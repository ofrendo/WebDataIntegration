%Yiru
%Take information from project abstract
\section{Introduction to Use Case and Datasets}
\subsection{Use Case}
In this project, our purpose is to integrate a dataset about companies with another dataset about cities their headquarters are located in. However, in order to gather more information about companies, first we combine several datasets together, which are all about companies but come from different sources. Then we will integrate this result with location. As such the resulting integrated dataset may be used for additional information regarding companies and their location around the world.

%not sure if we need this part
%First, we gather data from each data source. For most datasets we write queries for different web services(DBpedia and Freebase) to request data about companies and location. One dataset is provided as .xls file, so we transfer it into .csv file for mapping. Since a company might have abbreviation in each dataset, we do some transformation on name and country.\\
%In second phase, we identify a company in multiple datasets by their overlapping attributes. In order to reduce the comparing time, we use country as blocking key in most situations.\\
%Then, by using specific resolution strategies for each attribute, we solve the conflicting information about companies. In the end these datasets can be merged together and represented in the form of our integrated target schema.\\
%

\subsection{Datasets}
\subsubsection{Forbes}
Forbes is an American business magazine and it is well known for its lists and rankings, including its lists of the richest Americans (the Forbes 400) and rankings of world's top companies (the Forbes Global 2000). The ranking is based on a mix of four metrics: sales, profit, assets and market value. This dataset has 2000 entities and 11 attributes which mostly are about financial. Furthermore, this dataset contains official information, compared to DBpedia and Freebase which contains information less complete and less trustworthy.
\subsubsection{DBpedia}
DBpedia extracts structured information from Wikipedia. It offers a data dump, but here we may run into technical difficulties based on the size of the files (2.4GB compressed). Also, we would have to set up our own local server with a SPARQL endpoint. Thus, we use the public SPARQL endpoint(http://dbpedia.org/sparql) for accessing their data and also set some filters to get data we want, e.g. \texttt{numberOfEmployees} must higher then 100 or \texttt{populationTotal} is at least 10,000.
\subsubsection{Freebase}
Freebase was an online database composed by its community members, including individual, user-submitted wiki contributions. It offers a web service for quering data and a data dump (22GB zipped, 250GB uncompressed). Considering the size, we choose to write out own query. Unlike DBpedia, Freeabse use Metaweb query language (MQL) to access data. We also set filters, e.g. \texttt{number\_of\_employees} must have value.