\section{Data fusion}
\subsection{Input data}
%Oliver
Table \ref{tableAttributeDensities} contains the attribute densities and consistencies for each dataset: We used each of the four described in previous sections. One notable attribute is \texttt{countries}, which has a density of 1 in all datasets except for Freebase. This is due to the nature of Freebase: We query for company locations, which very often returns a city for which the country is not defined (see \footnote{http://www.freebase.com/m/0c0bbxc} for an example). The density of \texttt{locations} for Freebase shows a density of 1, supporting this. It also explains why a cross product blocking function returns more matches than partitioning by country when comparing Freebase to another dataset (see table \ref{tableBlockingFunctions}). Overall we raised the density especially of \texttt{industries}, \texttt{countries}, \texttt{locations}, \texttt{dateFounded} and \texttt{keyPeople} considering the high number of companies in the final fused dataset. On the other hand due to the relatively low number of correspondences that include the Forbes dataset the densities for \texttt{marketValue} and \texttt{continent} are accordingly low. 

%==================BEGIN TABLE

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Attribute} & \textbf{\begin{tabular}[c]{@{}c@{}}Forbes\\ (n=2000)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Freebase\\ (n=3182)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DBpedia\\ (n=16051)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Consist-\\ encies\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Fused\\ (n=6470)\end{tabular}} \\ \hline
name & 1,00 & 1,00 & 1,00 & 0,97 & 1,00 \\ \hline
countries & 1,00 & 0,40 & 1,00 & 1,00 & 1,00 \\ \hline
industries & 0,98 & 0,54 & 0,61 & 0,93 & 0,65 \\ \hline
revenue & 1,00 & 0,16 & 0,15 & 1,00 & 0,21 \\ \hline
\begin{tabular}[c]{@{}l@{}}numberOf-\\ Employees\end{tabular} & 0,00 & 1,00 & 0,32 & 1,00 & 0,38 \\ \hline
dateFounded & 0,00 & 0,81 & 0,79 & 0,99 & 0,82 \\ \hline
assets & 1,00 & 0,00 & 0,06 & 1,00 & 0,12 \\ \hline
marketValue & 1,00 & 0,00 & 0,00 & 1,00 & 0,06 \\ \hline
profit & 0,98 & 0,13 & 0,00 & 1,00 & 0,07 \\ \hline
continent & 1,00 & 0,00 & 0,00 & 1,00 & 0,06 \\ \hline
keyPeople & 0,00 & 0,31 & 0,55 & 0,97 & 0,59 \\ \hline
locations & 0,00 & 1,00 & 1,00 & 0,90 & 1,00 \\ \hline
\end{tabular}
\caption{Attribute densities and consistencies per dataset}
\label{tableAttributeDensities}
\end{table}

%==================END TABLE


For this project we decided to use the source as provenance data, as querying additional metadata (such as the author or the most recent date modified) for both Freebase and DBpedia would have made the data collection considerably more time consuming. We gave Forbes a higher data quality score than the others because we consider it to be more reliable and probably up to date compared to Freebase and DBpedia.




\subsection{Gold standard}
Our fused file contains five entities and each entity involves 15 attributes.For a company the revenue, numberOfEmployees, assets, marketValue and profit are always changing over time, in oder to get the latest data, we searched for different external sources. Take APPLE for example, we went to the homepage of APPLE  to read the latest financial statement\footnote{http://www.apple.com/pr/library/2015/10/27Apple-Reports-Record-Fourth-Quarter-Results.html} and to get the revenue. And then went to statista \footnote{http://www.statista.com/statistics/273439/number-of-employees-of-apple-since-2005/} to acquire numberOfEmployees. And we got assets, marketValue and keyPeople went from Forbes\footnote{http://www.forbes.com/companies/apple/}. By comparison, Forbes offers a higher quality data than Freebase and Dbpedia. Because of the same reason, we also searched for relatively fresh data on wikipedia\footnote{https://en.wikipedia.org} for some attributes of Location, such as population, area and elevation. In a nut shell, DBpedia offers an outdated data and updating frequency is a little low.
 
%Silvia/Zehui
%Size, content, how did you create it?

\subsection{Conflict resolution functions}
%Oliver
%Which was tried for each attribute?
%Very often straightforward and through logical reasoning
%In the end also chose a combination of functions for some attributes
%Own definitions, also own evaluation rules
%Include stuff from integrated schema again

The conflict resolution functions we tried and used for each attribute are listed in table \ref{tableConflictResolutionFunctions}. To begin with, the maximum accuracy we achieved with any approach was 0.94 due to some attributes not existing in any of the four datasets or because of very outdated values. The first notable difference between functions occurs for the attribute \texttt{industries}. The result of using an intersection between multiple datasets left very few values after fusing, because the industries were often very different. As such we decided to use a union and remove duplicates, thus leading to a more descriptive fused entity. Interestingly, the difference between using an intersection and a union is not as high for \texttt{keyPeople} as it is for \texttt{industries}. 
Next we tried both \textit{Max} and \textit{Average} for \texttt{numberOfEmployees}. Logically \textit{Max} should be better, since most companies should be growing, in the same sense that a profit should be positive. Looking deeper into this, the company IBM had a lower number of employees in reality than what was recorded in DBpedia, indicating an outdated value in DBpedia and thus leading to a lower accuracy when testing our gold standard. 


%===================== BEGIN TABLE

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Attributes Name} & \textbf{\begin{tabular}[c]{@{}c@{}}Datasets in which\\ the attribute is found\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Conflict resolution\\ function\end{tabular}} & \textbf{Accuracy} \\ \hline
\multirow{3}{*}{name} & \multirow{3}{*}{dataset 1, 2, 3} & FavourSources & 0,94 \\ \cline{3-4} 
 &  & Voting & 0,94 \\ \cline{3-4} 
 &  & LongestString & 0,94 \\ \hline
country & dataset 1, 2, 3, 4 & Voting & 0,94 \\ \hline
\multirow{2}{*}{industries} & \multirow{2}{*}{dataset 1, 2, 3} & Intersection & 0,88 \\ \cline{3-4} 
 &  & Union & 0,94 \\ \hline
revenue & dataset 1, 2, 3 & FavourSources & 0,94 \\ \hline
\multirow{2}{*}{numberOfEmployees} & \multirow{2}{*}{dataset 2, 3} & Average & 0,94 \\ \cline{3-4} 
 &  & Max & 0,93 \\ \hline
\multirow{3}{*}{dateFounded} & \multirow{3}{*}{dataset 2, 3} & \begin{tabular}[c]{@{}c@{}}MostComplete\\ (date)\end{tabular} & 0,94 \\ \cline{3-4} 
 &  & \begin{tabular}[c]{@{}c@{}}MostComplete\\ (sample)\end{tabular} & 0,94 \\ \cline{3-4} 
 &  & Combination & 0,94 \\ \hline
\multirow{2}{*}{assets} & \multirow{2}{*}{dataset 1, 2} & FavourSources & 0,93 \\ \cline{3-4} 
 &  & Max & 0,94 \\ \hline
\textit{marketValue} & \textit{dataset 1} & \textit{SingleSource} & \textit{0,94} \\ \hline
\multirow{2}{*}{profit} & \multirow{2}{*}{dataset 1, 3} & FavourSources & 0,94 \\ \cline{3-4} 
 &  & Max & 0,94 \\ \hline
\textit{continent} & \textit{dataset 1} & \textit{SingleSource} & \textit{0,94} \\ \hline
\multirow{2}{*}{keyPeople} & \multirow{2}{*}{dataset 2, 3} & Intersection & 0,93 \\ \cline{3-4} 
 &  & Union & 0,94 \\ \hline
\multirow{2}{*}{locationName} & \multirow{2}{*}{dataset 2, 3, 4} & \begin{tabular}[c]{@{}c@{}}Intersection+\\ MostComplete\end{tabular} & 0,93 \\ \cline{3-4} 
 &  & \begin{tabular}[c]{@{}c@{}}Union+\\ MostComplete\end{tabular} & 0,94 \\ \hline
\textit{population} & \textit{dataset 4} & \textit{SingleSource} & \textit{0,94} \\ \hline
\textit{area} & \textit{dataset 4} & \textit{SingleSource} & \textit{0,94} \\ \hline
\textit{elevation} & \textit{dataset 4} & \textit{SingleSource} & \textit{0,94} \\ \hline
\end{tabular}
\caption{Conflict resolution functions: Datasets 1, 2, 3 and 4 correspond to Forbes, DBpedia, Freebase and DBpedia locations}
\label{tableConflictResolutionFunctions}
\end{table}

%===================== END TABLE


When an attribute of the Forbes dataset was involved we tried a favored sources approach to confirm our assumption that the Forbes dataset is more reliable. However, considering the results for \texttt{assets}, for example, indicates that this is not always the case. After trying different methods for \texttt{dateFounded} we decided to use a combination by choosing the most complete date or the most complete record as a fallback. Using an intersection or union for \texttt{locations} seemed to show similar results compared to \texttt{keyPeople}. Lastly there are no obvious differences in accuracy in the functions used for \texttt{name}. This is due to the fact that very often differences in the name are due to the company type (such as "Inc."), which are filtered out. As such we decided to use \textit{LongestString} in order to keep entities as descriptive as possible.

\paragraph{}Conflict resolution functions we implemented included \textit{Intersection} and \textit{Union} for Strings because of the way \texttt{industries} and \texttt{keyPeople} were stored (delimited by ";;"), \textit{Max}, \textit{MostComplete (date)}, \textit{MostComplete (record)}, a combination of the two, \textit{Intersection} and \textit{Union} combined with \textit{MostComplete (record)}, and \textit{SingleSource}, which is applied when only one source possesses a given attribute.


% EVALUATION RULES
%Numeric attributes
%Because of the age of the Forbes dataset and the unreliability of DBpedia/Freebase a high max percentage difference was used to evaluate whether they were equal
%e.g. industry: Returns true if at least one of the companies have Levenshtein of over a certain threshold, e.g. 0.8







\section{Conclusion}
%Oliver
Looking back the datasets we collected from DBpedia and Freebase had sparse attributes. In addition to this the difficulty of collecting the right balance of data from Freebase and DBpedia made data collection a lengthy task. On the other hand once we had created the queries for the two datasets the flexibility of this method allowed us to select and extract exactly the attributes we wanted. As a result we had the opportunity to try many different identity and conflict resolution functions after collecting and mapping the data. During the phase of identity resolution we generated a relatively low number of correspondences of Freebase with Forbes, because many companies in Forbes are not available in our dataset. This is explained by many companies in Freebase not having a \texttt{NumberOfEmployees} attribute, for example. Changing this attribute to optional or removing it returns over 200.000 companies from Freebase, making the problem intractable. Lastly for the process of fusing our datasets we tried many different conflict resolution functions and implemented a few ourselves. As a result the fused dataset shows improved densities especially among shared attributes. Looking forward, a promising approach to improve accuracy would be to select metadata per attribute from DBpedia and Freebase such as the most recent date modified. This would then be used as a basis for conflict resolution functions relying on provenance data. 
















