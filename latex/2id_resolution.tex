%\newpage
\section{Identity resolution}
\subsection{Gold standards}
%For each of the subsubsections below:
%Content and size of your gold standard and procedure used to create it
%Give a few corner cases
%	companies that are very similar but not the same
%	companies that are not similar but are the same
As described above we have four datasets in total. We created three gold standards divided into a training set and a test set. Each test set's size is 10\% of the training set's size mostly including corner cases. For the convenience of matching, entities in smaller size dataset are compared to the bigger one and all gold standards are selected by stratified distribution in ascending order. The corner cases in our project are mainly entities which are very similar but not the same and entities that have different names but are the same entity.
%Silvia/Zehui
\paragraph{Forbes and Freebase} are compared by the shared attributes: company name, country, industries and revenue. The gold standard has 220 pairs in total, 120 of which are false and 100 of which are true. We labeled the types of our corner cases as Abbreviation, Incomplete name, Similar name and Same name with different countries or industries. Some examples of this are:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
%\item Bank of China vs Industrial and Commercial Bank of China  
\item Chevron vs Chevron Corporation $\longrightarrow true$
\item BP vs TNK-BP $\longrightarrow false$ 
\item Makita (U.S) vs Makita (Japan) $\longrightarrow false$
\end{itemize}


%Yiru
\paragraph{Freebase and DBpedia:} This gold standard has 200 pairs, including 110 false cases and 90 true cases. To create it we chose one company in Freebase and then searched for it in DBpedia. If they matched, we added it as true, otherwise we looked for one which has a similar name or equivalent values in other attributes, for example countries or industries. Some examples of corner cases are:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item Okinawa\_Electric\_Company vs Oki Electric Industry $\longrightarrow false$
\item E.ON\_Russia vs E.ON $\longrightarrow false$
\item Repsol vs Repsol YPF S.A. $\longrightarrow true$
\item Wacom\_(company) vs Wacom $\longrightarrow true$
\end{itemize}


%Silvia/Zehui
\paragraph{DBpedia companies and DBpedia locations} has two shared attributes: locationCity and locationCountry. Because city names extracted from DBpedia locations have multiple values due to multiple districts in one city, we defined the city without a specific district name as the value for integration. This gold standard has 270 pairs, 190 of which false and 80 of which are true. Two examples of corner cases are: 
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item New York vs New York City $\longrightarrow true$
\item New\_York\_City V.S. Syracuse, New York $\longrightarrow false$
\end{itemize}


\newpage
\subsection{Matching rules}
%Oliver
This section explains the matching rules we tried in order to generate correspondences accurately. We matched the following datasets with each other:
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
\item Forbes vs Freebase
\item Freebase vs DBpedia
\item DBpedia companies vs DBpedia locations
\end{itemize}

%%%%%%%%%%%%%%BEGIN TABLE

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Attribute} & \textbf{MatchingRule} & \textbf{P} & \textbf{R} & \textbf{F1} \\ \hline
\multicolumn{5}{|l|}{\textbf{Forbes vs Freebase}} \\ \hline
\multirow{2}{*}{name} & Equals & 1,0000 & 0,7500 & 0,8571 \\ \cline{2-5} 
 & Levenshtein & 0,8571 & 1,0000 & 0,9231 \\ \hline
\multirow{3}{*}{countries} & Equals & 0,8571 & 1,0000 & 0,9231 \\ \cline{2-5} 
 & Jaccard & 0,8571 & 1,0000 & 0,9231 \\ \cline{2-5} 
 & Highest Jaccard & 0,8571 & 1,0000 & 0,9231 \\ \hline
\multirow{2}{*}{industries} & Jaccard & 0,9091 & 0,8333 & 0,8696 \\ \cline{2-5} 
 & \begin{tabular}[c]{@{}l@{}}Combination of Jaccard\\ and Levenshtein\end{tabular} & 0,8571 & 1,0000 & 0,9231 \\ \hline
\begin{tabular}[c]{@{}l@{}}revenue/\\ profit\end{tabular} & \begin{tabular}[c]{@{}l@{}}PercentageSimilarity\\ (max\_percentage=0.5)\end{tabular} & 0,8571 & 1,0000 & 0,9231 \\ \hline
\multicolumn{5}{|l|}{\textbf{Freebase vs DBpedia}} \\ \hline
\begin{tabular}[c]{@{}l@{}}revenue/\\ numberOfEmployees\end{tabular} & \begin{tabular}[c]{@{}l@{}}PercentageSimilarity\\ (max\_percentage=0.5)\end{tabular} & 0,9167 & 0,9167 & 0,9167 \\ \hline
dateFounded & \begin{tabular}[c]{@{}l@{}}YearSimilarity\\ (maxDifference=20)\end{tabular} & 0,9167 & 0,9167 & 0,9167 \\ \hline
\multirow{2}{*}{keyPeople} & Jaccard & 0,9167 & 0,9167 & 0,9167 \\ \cline{2-5} 
 & \begin{tabular}[c]{@{}l@{}}Combination of Jaccard\\ and Levenshtein\end{tabular} & 0,9167 & 0,9167 & 0,9167 \\ \hline
\multirow{2}{*}{locations} & Jaccard & 0,9167 & 0,9167 & 0,9167 \\ \cline{2-5} 
 & Highest Jaccard & 0,9167 & 0,9167 & 0,9167 \\ \hline
\multicolumn{5}{|l|}{\textbf{DBpedia companies vs DBpedia locations}} \\ \hline
countries & Highest Jaccard & 0,9706 & 0,9429 & 0,9565 \\ \hline
\multirow{2}{*}{locations} & Jaccard & 0,9630 & 0,7429 & 0,8387 \\ \cline{2-5} 
 & Highest Jaccard & 0,9706 & 0,9429 & 0,9429 \\ \hline
\end{tabular}
\caption{Matching rule accuracies}
\label{my-label}
\end{table}

%%%%%%%%%%%%%%END TABLE

In particular the rules for \texttt{name}, \texttt{industries} and \texttt{locations} show different results. For \texttt{name} we chose to use Levenshtein because of misspellings, or because of the company type (e.g. "Inc." or "PLC"). However, this also introduces some problematic cases such as "West Japan Railway" and "East Japan Railway", which are different companies but possess very similar attribute values and also generate a very high Levenshtein similarity. For \texttt{industries} we tried Jaccard first. This however is not an accurate measure of similarity because of slight differences like "Transport" and "Transportation". As such we chose to use a combination of Jaccard and Levenshtein which led to better results:

$$sim_{Jaccard+Levenshtein} = \frac{\sum_{x,y} max(sim_{Levenshtein}(x,y))} {|x|+|y| - \sum_{x,y} max(sim_{Levenshtein}(x,y))}$$

To give an example of two companies with two industries each: "Computer, Transportation" and "Computers, Transport" would generate a similarity of 0 with Jaccard but 0.75 with our approach. We used the same approach for comparing \texttt{keyPeople}, where misspellings of names are more important. \texttt{locations} and \texttt{countries} were compared using \textit{Highest Jaccard}: This means we compared each location of an entity with each location of another entity using Jaccard and then picked the highest value. To give an example: Comparing a company with two locations "New York" and "London" with another company with only one location "New York City" would give bad results using \textit{Equals} or \textit{Levenshtein}, which is why we chose to use the highest Jaccard value. Very often there were entities with multiple countries or locations but only single intersections. Due to the sparsity and potential unreliability of Freebase and DBpedia we wanted the similarity to reflect this. Lastly we compared numeric attributes such as \texttt{Revenue} using the PercentageSimilarity: However numeric data from Freebase and DBpedia is too sparse, unreliable or outdated. Learning a matching rule via linear regression confirms this by assigning weights of 0 to both these attributes.




%HARD CASES
%Vodafone, VODAFONE PLC (TRUE)
%China Pacific Insurance, CHINA LIFE INSURANCE
%Central Japan Railway, EAST JAPAN RAILWAY (FALSE)
%Vodafone, Vodafone Group plc (TRUE)
%Syracuse, Utah AND Syracuse, New York
%"Chicago and Nashville" as single attribute from DBpedia

%EXAMPLE FOR HIGHEST JACCARD 
%	C1=New York, Chicago. C2 New York. Should be correspondence, so similarity = 1
%	Couldn't just take highest equals because of New York and New York City for example
	
	
\subsection{Blocking functions}
%Oliver
Table \ref{tableBlockingFunctions} shows the blocking functions we tried and used in our project. For the comparison of the Forbes and Freebase datasets a partitioning by \texttt{countries} shows good results, which is consistent with the high density of the attribute in both datasets. We also tried a sorted neighborhood approach on the same attribute which seemed to be less effective. Using a cross product approach for comparing Freebase with DBpedia was impossible due to the large size of DBpedia. As such we tried partitioning by \texttt{countries}, \texttt{dateFounded} (where the blocking key is $year/20$) and a combination of the two. The combination reflects our own implementation of a partitioning blocker, where we generate a match to be evaluated if the one of the two blocking keys are the same. This shows the best results because both attributes are relatively, but not completely, dense in both datasets, which is why the reduction ratio is lower then when using only one of the two. When comparing companies with locations from the DBpedia datasets \texttt{countries} is the only possible blocking key. 

%%%%%%%%%%%%%%BEGIN TABLE

\begin{table}[H]
\centering
\begin{tabular}{|l|l|c|c|c|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Dataset \\ Comparison\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Blocking \\ function\end{tabular}}} & \textbf{Time} & \textbf{Match} & \textbf{Ratio} & \multicolumn{1}{c|}{\textbf{P}} & \multicolumn{1}{c|}{\textbf{R}} & \multicolumn{1}{c|}{\textbf{F1}} \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Forbes vs \\ Freebase\end{tabular}} & CrossProduct & 00:32 & 509 & 1,00 & \multicolumn{1}{c|}{0,86} & \multicolumn{1}{c|}{1,00} & \multicolumn{1}{c|}{0,92} \\ \cline{2-8} 
 & \begin{tabular}[c]{@{}l@{}}SortedNeigh.\\ (Country)\end{tabular} & 00:05 & 319 & 6,80 & \multicolumn{1}{c|}{0,87} & \multicolumn{1}{c|}{0,58} & \multicolumn{1}{c|}{0,70} \\ \cline{2-8} 
 & \begin{tabular}[c]{@{}l@{}}Partitioning\\ (Country)\end{tabular} & 00:02 & 425 & 20,19 & 0,86 & 1,00 & 0,92 \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Freebase vs \\ DBpedia\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Partitioning\\ (Country)\end{tabular} & 00:44 & 576 & 15,92 & 0,90 & 0,75 & 0,82 \\ \cline{2-8} 
 & \begin{tabular}[c]{@{}l@{}}Partitioning\\ (DateFounded)\end{tabular} & 00:39 & 496 & 9,43 & 0,89 & 0,67 & 0,76 \\ \cline{2-8} 
 & \begin{tabular}[c]{@{}l@{}}Partitioning\\ (Combination)\end{tabular} & 01:22 & 671 & 6,13 & 0,92 & 0,92 & 0,92 \\ \hline
\begin{tabular}[c]{@{}l@{}}Companies\\ vs\\ Locations\end{tabular} & \begin{tabular}[c]{@{}l@{}}Partitioning\\ (Country)\end{tabular} & 00:41 & 7.921 & 4,11 & 0,97 & 0,94 & 0,96 \\ \hline
\end{tabular}
\caption{Blocking functions}
\label{tableBlockingFunctions}
\end{table}

%%%%%%%%%%%%%%END TABLE

%Notes on blocking function:
%Freebase vs Forbes: 
%- Country is always available in Forbes, sparse in Freebase (EXACT NUMBER)

%Freebase vs DBPedia:
%- Country is somewhat sparse in both datasets (EXACT NUMBER)
%- DateFounded somewhat sparse in both (EXACT NUMBER)

%Company vs Location
%- Country is only way to go

%Own idea: Own implementation of partitioning blocker, but more fuzzy logic
%- I.E. Country first, then date, then location or so as alternative (EXACT NUMBER)
%- Generate more pairs, matching rule will take care if its same company or not- more conservative approach


\subsection{Learning matching rules}
%Oliver
We were able to improve the results of our identity resolution by learning the weights for a linear matching rule from a linear regression in RapidMiner over our handwritten rules. To give an example, the learned weights for the datasets from Freebase and DBpedia are as follows. Interestingly, both \texttt{keyPeople} and \texttt{locations} seem to be important,  while the weights for both numeric attributes \texttt{revenue} and \texttt{numberOfEmployees} is assigned a weight of 0, indicating the attributes are not very useful for an accurate comparison. Lastly the \texttt{name} attribute has the highest weight, as expected.

\begin{center}
\begin{tabular}{lrlr}
name & 0.689       & revenue & 0.000\\
countries & 0.088  & numberOfEmployees & 0.000\\
industries & 0.025 & keyPeople & 0.377\\
dateFounded & 0.170& locations & 0.218\\
\textit{intercept} & -0.135 &  & 
\end{tabular}
\end{center}

%\begin{array}
%x & y \\
%z & w 
%\end{array}
%


%Using linear regression etc
%Why was this better than handwritten one
%Discussion of both learned rules







